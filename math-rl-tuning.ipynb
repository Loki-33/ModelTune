{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13500691,"sourceType":"datasetVersion","datasetId":8572047},{"sourceId":619322,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":465804,"modelId":481629},{"sourceId":619389,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":465862,"modelId":481687}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade transformers huggingface_hub --q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nimport json \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import LoraConfig, TaskType, get_peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:30.217756Z","iopub.execute_input":"2025-11-02T12:12:30.218055Z","iopub.status.idle":"2025-11-02T12:12:30.222206Z","shell.execute_reply.started":"2025-11-02T12:12:30.218036Z","shell.execute_reply":"2025-11-02T12:12:30.221468Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:35.989101Z","iopub.execute_input":"2025-11-02T12:12:35.989427Z","iopub.status.idle":"2025-11-02T12:12:35.993494Z","shell.execute_reply.started":"2025-11-02T12:12:35.989405Z","shell.execute_reply":"2025-11-02T12:12:35.992703Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model_name=\"EleutherAI/pythia-410m\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:35.994770Z","iopub.execute_input":"2025-11-02T12:12:35.995119Z","iopub.status.idle":"2025-11-02T12:12:36.097423Z","shell.execute_reply.started":"2025-11-02T12:12:35.995099Z","shell.execute_reply":"2025-11-02T12:12:36.096415Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(model_name,attn_implementation=\"sdpa\",dtype=torch.float16, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:39.192805Z","iopub.execute_input":"2025-11-02T12:12:39.193624Z","iopub.status.idle":"2025-11-02T12:12:48.523396Z","shell.execute_reply.started":"2025-11-02T12:12:39.193597Z","shell.execute_reply":"2025-11-02T12:12:48.522602Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"141701928a8049288d765e7f5e2585d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c93a9b2b44e14125aaa6c0696fff1af5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06377818635f49a5bbf8d353a04c16c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024d82926ba148ecb55fbee9014f152a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62dcf47957d442e9f471d1d57b8eb41"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.padding_side = 'left'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.524480Z","iopub.execute_input":"2025-11-02T12:12:48.524726Z","iopub.status.idle":"2025-11-02T12:12:48.529105Z","shell.execute_reply.started":"2025-11-02T12:12:48.524707Z","shell.execute_reply":"2025-11-02T12:12:48.528232Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from peft import PeftModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.530080Z","iopub.execute_input":"2025-11-02T12:12:48.530847Z","iopub.status.idle":"2025-11-02T12:12:48.566318Z","shell.execute_reply.started":"2025-11-02T12:12:48.530823Z","shell.execute_reply":"2025-11-02T12:12:48.565580Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"policy_model = PeftModel.from_pretrained(\n    base_model,\n    \"/kaggle/input/loraadapters/pytorch/default/1\",\n    trainable=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.568118Z","iopub.execute_input":"2025-11-02T12:12:48.568664Z","iopub.status.idle":"2025-11-02T12:12:48.748887Z","shell.execute_reply.started":"2025-11-02T12:12:48.568643Z","shell.execute_reply":"2025-11-02T12:12:48.748014Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# config = LoraConfig(\n#     r=8,\n#     lora_alpha=16,\n#     lora_dropout=0.1,\n#     bias='none',\n#     target_modules=[\"query_key_value\"],\n#     task_type=TaskType.CAUSAL_LM\n# )\n# lora_model = get_peft_model(model, config)\n# optimizer = torch.optim.AdamW(lora_model.parameters(), lr=1e-4)\n\n# lora_model, optimizer = accelerator.prepare(lora_model, optimizer)\n\n# accelerator.load_state(\"/kaggle/input/bestmodel/pytorch/default/1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.749771Z","iopub.execute_input":"2025-11-02T12:12:48.750281Z","iopub.status.idle":"2025-11-02T12:12:48.754039Z","shell.execute_reply.started":"2025-11-02T12:12:48.750232Z","shell.execute_reply":"2025-11-02T12:12:48.753181Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# lora_model.save_pretrained(\"/kaggle/working/lora_adapters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.754787Z","iopub.execute_input":"2025-11-02T12:12:48.755064Z","iopub.status.idle":"2025-11-02T12:12:48.764654Z","shell.execute_reply.started":"2025-11-02T12:12:48.755032Z","shell.execute_reply":"2025-11-02T12:12:48.764027Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"value_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:48.765458Z","iopub.execute_input":"2025-11-02T12:12:48.765935Z","iopub.status.idle":"2025-11-02T12:12:50.004843Z","shell.execute_reply.started":"2025-11-02T12:12:48.765909Z","shell.execute_reply":"2025-11-02T12:12:50.003867Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-410m and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"value_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias='none',\n    task_type=TaskType.SEQ_CLS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:50.006066Z","iopub.execute_input":"2025-11-02T12:12:50.006395Z","iopub.status.idle":"2025-11-02T12:12:50.013374Z","shell.execute_reply.started":"2025-11-02T12:12:50.006366Z","shell.execute_reply":"2025-11-02T12:12:50.012386Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"value_model = get_peft_model(value_model, value_config)\nvalue_model.to('cuda')\nvalue_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:50.014088Z","iopub.execute_input":"2025-11-02T12:12:50.014316Z","iopub.status.idle":"2025-11-02T12:12:53.542883Z","shell.execute_reply.started":"2025-11-02T12:12:50.014298Z","shell.execute_reply":"2025-11-02T12:12:53.541997Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"value_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:53.546690Z","iopub.execute_input":"2025-11-02T12:12:53.547469Z","iopub.status.idle":"2025-11-02T12:12:54.044998Z","shell.execute_reply.started":"2025-11-02T12:12:53.547438Z","shell.execute_reply":"2025-11-02T12:12:54.044106Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,573,888 || all params: 355,397,632 || trainable%: 0.4429\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"value_model.device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:54.046172Z","iopub.execute_input":"2025-11-02T12:12:54.046468Z","iopub.status.idle":"2025-11-02T12:12:54.983189Z","shell.execute_reply.started":"2025-11-02T12:12:54.046449Z","shell.execute_reply":"2025-11-02T12:12:54.981739Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"ref_model = PeftModel.from_pretrained(\n    base_model,\n    \"/kaggle/input/loraadapters/pytorch/default/1\",\n    trainable=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:54.984047Z","iopub.execute_input":"2025-11-02T12:12:54.984338Z","iopub.status.idle":"2025-11-02T12:12:55.665710Z","shell.execute_reply.started":"2025-11-02T12:12:54.984311Z","shell.execute_reply":"2025-11-02T12:12:55.664928Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"with open(\"/kaggle/input/mathdataset/math_dataset.json\", 'r') as f:\n    dataset = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:55.666489Z","iopub.execute_input":"2025-11-02T12:12:55.666997Z","iopub.status.idle":"2025-11-02T12:12:55.677159Z","shell.execute_reply.started":"2025-11-02T12:12:55.666977Z","shell.execute_reply":"2025-11-02T12:12:55.676567Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:55.677928Z","iopub.execute_input":"2025-11-02T12:12:55.678229Z","iopub.status.idle":"2025-11-02T12:12:56.405502Z","shell.execute_reply.started":"2025-11-02T12:12:55.678213Z","shell.execute_reply":"2025-11-02T12:12:56.404702Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_data = []\nfor item in dataset['train']:\n    train_data.append({\n        'question': item['question'],\n        'answer': item['answer']\n    })\n\ntrain_dataset = Dataset.from_list(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:12:56.406336Z","iopub.execute_input":"2025-11-02T12:12:56.406903Z","iopub.status.idle":"2025-11-02T12:12:56.420672Z","shell.execute_reply.started":"2025-11-02T12:12:56.406884Z","shell.execute_reply":"2025-11-02T12:12:56.419863Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def tokenize_function(sample):\n    tokenized = tokenizer(\n        sample['question'], \n        truncation=True, \n        padding='max_length',\n        max_length=256\n    )\n    return {\n        'input_ids': tokenized['input_ids'],\n        'attention_mask': tokenized['attention_mask'],\n        'answer': sample['answer'],  # Keep this!\n        'question': sample['question']  # Keep this too!\n    }\n\ntokenized_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=['question', 'answer']  # Remove only after copying\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:01.838135Z","iopub.execute_input":"2025-11-02T12:13:01.838757Z","iopub.status.idle":"2025-11-02T12:13:02.165427Z","shell.execute_reply.started":"2025-11-02T12:13:01.838732Z","shell.execute_reply":"2025-11-02T12:13:02.164758Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/738 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e838dbef5e204a78a34664ab22984dd6"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:02.359732Z","iopub.execute_input":"2025-11-02T12:13:02.360390Z","iopub.status.idle":"2025-11-02T12:13:02.365310Z","shell.execute_reply.started":"2025-11-02T12:13:02.360361Z","shell.execute_reply":"2025-11-02T12:13:02.364518Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n    num_rows: 738\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from typing import Callable, Optional, Tuple, List\nfrom transformers import PreTrainedTokenizerBase\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:02.629621Z","iopub.execute_input":"2025-11-02T12:13:02.629933Z","iopub.status.idle":"2025-11-02T12:13:02.634233Z","shell.execute_reply.started":"2025-11-02T12:13:02.629910Z","shell.execute_reply":"2025-11-02T12:13:02.633550Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def compute_reward(response: torch.Tensor, answers: List)->torch.Tensor:\n    rewards = []\n    for response, answer in zip(response, answers):\n        response_ = response.lower().strip()\n        answer_ = answer.lower().strip()\n\n        if answer_ in response_:\n            reward = 1.0\n        else:\n            reward= -1.0\n        rewards.append(reward)\n    return torch.tensor(rewards, dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:04.589651Z","iopub.execute_input":"2025-11-02T12:13:04.589903Z","iopub.status.idle":"2025-11-02T12:13:04.595219Z","shell.execute_reply.started":"2025-11-02T12:13:04.589886Z","shell.execute_reply":"2025-11-02T12:13:04.594452Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def compute_gae(reward: torch.Tensor, values:torch.Tensor, gamma: float=0.99, lam: float=0.95)->Tuple[torch.Tensor, torch.Tensor]:\n    if values.dim() == 1:\n        advantages = reward - values  # (batch_size,)\n        returns = reward  # (batch_size,)\n        return advantages.unsqueeze(-1), returns.unsqueeze(-1)\n \n    batch_size, seq_len = values.shape\n    rewards = torch.zeros_like(values)\n    rewards[:, -1] = reward\n\n    next_values = torch.zeros_like(values)\n    next_values[:, :-1] = values[:,1:]\n    deltas = rewards+gamma * next_values - values\n    advantages = torch.zeros_like(values)\n    gae = torch.zeros(batch_size).to(values.device)\n    for t in reversed(range(seq_len)):\n        gae = deltas[:, t]+gamma*lam*gae\n        advantages[:, t] = gae\n\n    returns = advantages + values \n    return advantages, returns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:04.779318Z","iopub.execute_input":"2025-11-02T12:13:04.779877Z","iopub.status.idle":"2025-11-02T12:13:04.785819Z","shell.execute_reply.started":"2025-11-02T12:13:04.779855Z","shell.execute_reply":"2025-11-02T12:13:04.784945Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def get_log_probs(model, input_ids, attention_mask):\n    \"\"\"Get log probabilities for generated tokens\"\"\"\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    logits = outputs.logits[:, :-1, :]\n    log_probs = F.log_softmax(logits, dim=-1)\n    \n    next_tokens = input_ids[:, 1:]\n    token_log_probs = torch.gather(log_probs, 2, next_tokens.unsqueeze(-1)).squeeze(-1)\n    \n    mask = attention_mask[:, 1:].float()\n    token_log_probs = token_log_probs * mask\n    \n    return token_log_probs, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:04.908960Z","iopub.execute_input":"2025-11-02T12:13:04.909235Z","iopub.status.idle":"2025-11-02T12:13:04.914459Z","shell.execute_reply.started":"2025-11-02T12:13:04.909214Z","shell.execute_reply":"2025-11-02T12:13:04.913604Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"for n, p in policy_model.named_parameters():\n    if 'lora' in n:\n        p.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:07.599125Z","iopub.execute_input":"2025-11-02T12:13:07.599852Z","iopub.status.idle":"2025-11-02T12:13:07.605105Z","shell.execute_reply.started":"2025-11-02T12:13:07.599827Z","shell.execute_reply":"2025-11-02T12:13:07.604418Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"policy_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:07.606302Z","iopub.execute_input":"2025-11-02T12:13:07.606515Z","iopub.status.idle":"2025-11-02T12:13:07.619700Z","shell.execute_reply.started":"2025-11-02T12:13:07.606500Z","shell.execute_reply":"2025-11-02T12:13:07.618917Z"}},"outputs":[{"name":"stdout","text":"trainable params: 786,432 || all params: 406,120,448 || trainable%: 0.1936\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def compute_entropy(logits, mask):\n    probs = F.softmax(logits, dim=-1)\n    log_probs = F.log_softmax(logits, dim=-1)\n    entropy = -(probs * log_probs).sum(dim=-1)\n    return (entropy * mask).sum(dim=-1) / mask.sum(dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:07.717204Z","iopub.execute_input":"2025-11-02T12:13:07.717692Z","iopub.status.idle":"2025-11-02T12:13:07.721987Z","shell.execute_reply.started":"2025-11-02T12:13:07.717669Z","shell.execute_reply":"2025-11-02T12:13:07.721109Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def compute_reward(responses: List[str], answers: List[str]) -> torch.Tensor:\n    \"\"\"Better reward with partial credit\"\"\"\n    from difflib import SequenceMatcher\n    \n    rewards = []\n    for response, answer in zip(responses, answers):\n        response_clean = response.lower().strip()\n        answer_clean = answer.lower().strip()\n        \n        # Exact match in response\n        if answer_clean in response_clean:\n            reward = 1.0\n        \n        # Partial match - string similarity\n        elif SequenceMatcher(None, answer_clean, response_clean).ratio() > 0.6:\n            reward = 0.5\n        \n        # Answer words present\n        elif any(word in response_clean for word in answer_clean.split() if len(word) > 2):\n            reward = 0.2\n        \n        # At least it tried (not empty)\n        elif len(response_clean) > 10:\n            reward = -0.3  # Small penalty, not harsh\n        \n        else:\n            reward = -1.0\n        response_len = len(response_clean.split())\n        answer_len = len(answer_clean.split())\n\n        if response_len > 0:\n            len_ratio = answer_len / response_len\n            len_ratio = max(0.0, min(len_ratio, 1.5))\n            reward += 0.3 * len_ratio  # weight conciseness\n            \n        rewards.append(reward)\n    \n    return torch.tensor(rewards, dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:09.209089Z","iopub.execute_input":"2025-11-02T12:13:09.209393Z","iopub.status.idle":"2025-11-02T12:13:09.215991Z","shell.execute_reply.started":"2025-11-02T12:13:09.209375Z","shell.execute_reply":"2025-11-02T12:13:09.215222Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def ppo_trainer(\n    policy_model: PeftModel, \n    value_model: PeftModel,\n    ref_model: PeftModel,\n    tokenizer: PreTrainedTokenizerBase,\n    dataset: Dataset,\n    policy_optimizer: torch.optim.AdamW,\n    value_optimizer: torch.optim.AdamW,\n    reward_model: Optional[Callable[[list[str], list[str]], list[float]]] = None,\n    num_epochs=3,\n    rollout_batch_size=32,\n    mini_batch_size=8,\n    ppo_epochs=4,\n    clip_eps: float=0.1,\n    kl_coef: float=0.01,\n    value_coef: float=0.5,\n    entropy_coef: float=0.01,\n    gamma: float=0.99,\n    lam: float=0.95,\n    max_gen_len: int=128\n):\n    def collate_fn(batch):\n        input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n        attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n        answers = [item['answer'] for item in batch]\n        questions = [item['question'] for item in batch]\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'answers': answers,\n            'questions': questions\n        }\n        \n    dataloader = DataLoader(dataset, batch_size=rollout_batch_size, collate_fn=collate_fn,shuffle=True)\n    all_epoch_rewards = []\n    \n\n    for epoch in range(num_epochs):\n        print('-----------------------------')\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        epoch_rewards = []\n        for b_idx, batch in enumerate(tqdm(dataloader, desc='Rollouts')):\n            policy_model.eval()\n            value_model.eval()\n            ref_model.eval()\n            \n            rollout_buffer=[]\n            with torch.no_grad():\n                input_ids = batch['input_ids'].to(policy_model.device)\n                attention_mask = batch['attention_mask'].to(policy_model.device)\n                answers = batch['answers']\n                \n                response_token = policy_model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_new_tokens=max_gen_len,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    pad_token_id=tokenizer.pad_token_id\n                )\n                responses=tokenizer.batch_decode(response_token, skip_special_tokens=True)\n                \n                rewards = compute_reward(responses, answers)\n                rewards = rewards.to(policy_model.device)\n                batch_avg_reward = rewards.mean().item()\n                epoch_rewards.append(batch_avg_reward)\n\n                attention_mask = (response_token != tokenizer.pad_token_id).long()\n\n                values = value_model(\n                    input_ids=response_token,\n                    attention_mask=attention_mask\n                ).logits.squeeze(-1)\n\n                old_log_probs, mask = get_log_probs(\n                    policy_model, response_token, attention_mask\n                )\n                ref_log_probs,_ = get_log_probs(\n                    ref_model, response_token, attention_mask\n                )\n                advantages, returns = compute_gae(rewards, values, gamma, lam)\n                advantages = (advantages-advantages.mean()) / (advantages.std()+1e-8)\n\n                for i in range(len(batch['answers'])):\n                    rollout_buffer.append({\n                        'input_ids': response_token[i].cpu(),\n                        'attention_mask': attention_mask[i].cpu(),\n                        'old_log_probs': old_log_probs[i].cpu(),\n                        'ref_log_probs': ref_log_probs[i].cpu(),\n                        'advantages': advantages[i].cpu(),\n                        'returns': returns[i].cpu(),\n                        'mask': mask[i].cpu()\n                    })\n            policy_model.train()\n            value_model.train()\n\n            for ppo_epoch in range(ppo_epochs):\n                import random\n                random.shuffle(rollout_buffer)\n                num_minibatches = len(rollout_buffer)//mini_batch_size\n\n                ppo_policy_losses =[]\n                ppo_value_losses =[]\n                ppo_kl_penalties = []\n                ppo_entropies = []\n                for mb_idx in range(num_minibatches):\n                    start_idx = mb_idx* mini_batch_size\n                    end_idx = start_idx + mini_batch_size \n                    minibatch = rollout_buffer[start_idx:end_idx]\n                    \n                    #stack minibatches\n                    mb_input_ids = torch.stack([item['input_ids']for item in minibatch])\n                    mb_attention_mask = torch.stack([item['attention_mask'] for item in minibatch])\n                    mb_old_log_probs = torch.stack([item['old_log_probs'] for item in minibatch])\n                    mb_ref_log_probs = torch.stack([item['ref_log_probs'] for item in minibatch])\n                    mb_advantages = torch.stack([item['advantages'] for item in minibatch])\n                    mb_returns = torch.stack([item['returns'] for item in minibatch])\n                    mb_mask = torch.stack([item['mask'] for item in minibatch])\n\n                    #move to device\n                    mb_input_ids = mb_input_ids.to(policy_model.device)\n                    mb_attention_mask = mb_attention_mask.to(policy_model.device)\n                    mb_advantages = mb_advantages.to(policy_model.device)\n                    mb_old_log_probs = mb_old_log_probs.to(policy_model.device)\n                    mb_ref_log_probs = mb_ref_log_probs.to(policy_model.device)\n                    mb_returns = mb_returns.to(policy_model.device)\n                    mb_mask = mb_mask.to(policy_model.device)\n                    \n                    policy_optimizer.zero_grad()\n                    new_log_probs, _ = get_log_probs(\n                        policy_model,\n                        mb_input_ids,\n                        mb_attention_mask\n                    )\n                    \n                    #PPO loss\n                    ratio = torch.exp(new_log_probs - mb_old_log_probs)    \n                    surr1 = ratio*mb_advantages*mb_mask\n                    surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps)* mb_advantages * mb_mask\n                    policy_loss = -torch.min(surr1, surr2).sum() / mb_mask.sum()\n                    #KL \n                    kl_div = (mb_ref_log_probs.exp() * (mb_ref_log_probs - new_log_probs)) * mb_mask\n                    kl_penalty = kl_div.sum() / mb_mask.sum()\n                    #entropy\n                    outputs = policy_model(\n                        input_ids=mb_input_ids,\n                        attention_mask=mb_attention_mask\n                    )\n                    \n                    entropy=compute_entropy(outputs.logits[:, :-1, :], mb_mask)\n\n                    total_policy_loss = policy_loss + kl_coef * kl_penalty - entropy_coef * entropy.mean()\n                    #backward\n                    if not torch.isnan(total_policy_loss):\n                        total_policy_loss.backward()\n                        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n                        policy_optimizer.step()\n\n                    #value update\n                    value_optimizer.zero_grad()\n                    new_values = value_model(\n                        input_ids=mb_input_ids,\n                        attention_mask=mb_attention_mask\n                    ).logits.squeeze(-1)\n                    \n                    # if new_values.dim() == 1:\n                    #     seq_len = mb_input_ids.shape[1]\n                    #     new_values = new_values.unsqueeze(-1).expand(-1, seq_len)\n                    final_returns = mb_returns[:, -1]\n                    #value loss\n                    value_loss = value_coef * ((new_values.squeeze() - mb_returns.squeeze()) ** 2).mean()\n\n                    # Backward and optimize\n                    if not torch.isnan(value_loss):\n                        value_loss.backward()\n                        torch.nn.utils.clip_grad_norm_(value_model.parameters(), max_norm=1.0)\n                        value_optimizer.step()\n\n                    ppo_policy_losses.append(policy_loss.item())\n                    ppo_value_losses.append(value_loss.item())\n                    ppo_kl_penalties.append(kl_penalty.item())\n                    ppo_entropies.append(entropy.mean().item())\n                \n                if b_idx % 10 == 0 and len(ppo_policy_losses) > 0:\n                    print(f\"\\n  Batch {b_idx} - PPO Epoch {ppo_epoch+1}/{ppo_epochs}\")\n                    print(f\"    Policy Loss: {sum(ppo_policy_losses)/len(ppo_policy_losses):.4f}\")\n                    print(f\"    Value Loss: {sum(ppo_value_losses)/len(ppo_value_losses):.4f}\")\n                    print(f\"    KL Div: {sum(ppo_kl_penalties)/len(ppo_kl_penalties):.4f}\")\n                    print(f\"    Entropy: {sum(ppo_entropies)/len(ppo_entropies):.4f}\")\n                    print(f\"    Avg Reward: {batch_avg_reward:.4f}\")\n            \n            del rollout_buffer\n            torch.cuda.empty_cache()     \n            \n        epoch_avg_reward = sum(epoch_rewards) / len(epoch_rewards)\n        all_epoch_rewards.append(epoch_avg_reward)\n        print(f\"\\n{'='*50}\")\n        print(f\"Epoch {epoch+1} Average Reward: {epoch_avg_reward:.4f}\")\n        print(f\"{'='*50}\\n\")\n    \n    return policy_model, value_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:09.575442Z","iopub.execute_input":"2025-11-02T12:13:09.575737Z","iopub.status.idle":"2025-11-02T12:13:09.597170Z","shell.execute_reply.started":"2025-11-02T12:13:09.575715Z","shell.execute_reply":"2025-11-02T12:13:09.596385Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-4)\nvalue_optimizer = torch.optim.AdamW(value_model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:14.990346Z","iopub.execute_input":"2025-11-02T12:13:14.991121Z","iopub.status.idle":"2025-11-02T12:13:14.998780Z","shell.execute_reply.started":"2025-11-02T12:13:14.991096Z","shell.execute_reply":"2025-11-02T12:13:14.997824Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING']='1'\nos.environ[\"TORCH_USE_CUDA_DSA\"] = '1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:15.000170Z","iopub.execute_input":"2025-11-02T12:13:15.000503Z","iopub.status.idle":"2025-11-02T12:13:15.013040Z","shell.execute_reply.started":"2025-11-02T12:13:15.000483Z","shell.execute_reply":"2025-11-02T12:13:15.012307Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"trained_policy, trained_value = ppo_trainer(\n    policy_model=policy_model,\n    value_model=value_model,\n    ref_model=ref_model,\n    tokenizer=tokenizer,\n    dataset=tokenized_dataset,\n    policy_optimizer=policy_optimizer,\n    value_optimizer=value_optimizer,\n    reward_model=None,\n    num_epochs=8,  \n    rollout_batch_size=16,  \n    mini_batch_size=4,\n    ppo_epochs=4,\n    clip_eps=0.1,\n    kl_coef=0.3,\n    value_coef=0.5,\n    entropy_coef=0.01,\n    gamma=0.85,\n    lam=0.87,\n    max_gen_len=128 \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:13:27.004295Z","iopub.execute_input":"2025-11-02T12:13:27.005209Z","iopub.status.idle":"2025-11-02T14:41:59.561179Z","shell.execute_reply.started":"2025-11-02T12:13:27.005181Z","shell.execute_reply":"2025-11-02T14:41:59.560513Z"}},"outputs":[{"name":"stdout","text":"-----------------------------\nEpoch 1/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.1023\n    Value Loss: 1.2231\n    KL Div: 0.0027\n    Entropy: 1.7889\n    Avg Reward: -0.2134\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: 0.0887\n    Value Loss: 0.5439\n    KL Div: 0.0077\n    Entropy: 1.8084\n    Avg Reward: -0.2134\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0399\n    Value Loss: 0.1944\n    KL Div: 0.0112\n    Entropy: 1.8183\n    Avg Reward: -0.2134\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:53, 24.64s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: 0.0457\n    Value Loss: 0.1589\n    KL Div: 0.0097\n    Entropy: 1.8161\n    Avg Reward: -0.2134\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:02<14:55, 24.20s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0094\n    Value Loss: 0.5084\n    KL Div: 0.0036\n    Entropy: 1.3603\n    Avg Reward: -0.2139\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0059\n    Value Loss: 0.1620\n    KL Div: 0.0049\n    Entropy: 1.3548\n    Avg Reward: -0.2139\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: -0.0007\n    Value Loss: 0.0411\n    KL Div: 0.0040\n    Entropy: 1.3496\n    Avg Reward: -0.2139\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:26<14:31, 24.20s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0026\n    Value Loss: 0.1277\n    KL Div: 0.0046\n    Entropy: 1.3409\n    Avg Reward: -0.2139\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:03<10:52, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0114\n    Value Loss: 0.3704\n    KL Div: 0.0071\n    Entropy: 1.1497\n    Avg Reward: -0.2141\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0112\n    Value Loss: 0.1183\n    KL Div: 0.0066\n    Entropy: 1.1522\n    Avg Reward: -0.2141\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: 0.0097\n    Value Loss: 0.0531\n    KL Div: 0.0096\n    Entropy: 1.1480\n    Avg Reward: -0.2141\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:28<10:29, 24.20s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0080\n    Value Loss: 0.0732\n    KL Div: 0.0121\n    Entropy: 1.1588\n    Avg Reward: -0.2141\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:05<06:51, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0580\n    Value Loss: 0.1854\n    KL Div: 0.0124\n    Entropy: 1.0736\n    Avg Reward: -0.0509\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: 0.0909\n    Value Loss: 0.0774\n    KL Div: 0.0098\n    Entropy: 1.0791\n    Avg Reward: -0.0509\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: 0.0921\n    Value Loss: 0.0513\n    KL Div: 0.0206\n    Entropy: 1.0709\n    Avg Reward: -0.0509\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:30<06:27, 24.20s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: 0.0576\n    Value Loss: 0.0572\n    KL Div: 0.0148\n    Entropy: 1.0721\n    Avg Reward: -0.0509\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:07<02:49, 24.17s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0132\n    Value Loss: 0.1203\n    KL Div: 0.0143\n    Entropy: 0.7951\n    Avg Reward: -0.2145\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0112\n    Value Loss: 0.0599\n    KL Div: 0.0243\n    Entropy: 0.8126\n    Avg Reward: -0.2145\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: 0.0082\n    Value Loss: 0.0380\n    KL Div: 0.0253\n    Entropy: 0.8019\n    Avg Reward: -0.2145\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:31<02:25, 24.17s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0026\n    Value Loss: 0.0229\n    KL Div: 0.0146\n    Entropy: 0.7843\n    Avg Reward: -0.2145\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:37<00:00, 23.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 1 Average Reward: -0.1822\n==================================================\n\n-----------------------------\nEpoch 2/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0173\n    Value Loss: 0.0491\n    KL Div: 0.0085\n    Entropy: 0.4824\n    Avg Reward: -0.2954\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: 0.0082\n    Value Loss: 0.0602\n    KL Div: 0.0065\n    Entropy: 0.5063\n    Avg Reward: -0.2954\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0076\n    Value Loss: 0.0198\n    KL Div: 0.0097\n    Entropy: 0.5245\n    Avg Reward: -0.2954\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:32, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: 0.0088\n    Value Loss: 0.0202\n    KL Div: 0.0290\n    Entropy: 0.5470\n    Avg Reward: -0.2954\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:01<14:52, 24.12s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0101\n    Value Loss: 0.0347\n    KL Div: 0.0053\n    Entropy: 0.1906\n    Avg Reward: -0.2954\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0151\n    Value Loss: 0.0174\n    KL Div: 0.0063\n    Entropy: 0.2053\n    Avg Reward: -0.2954\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0111\n    Value Loss: 0.0196\n    KL Div: 0.0090\n    Entropy: 0.2017\n    Avg Reward: -0.2954\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:25<14:28, 24.12s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0098\n    Value Loss: 0.0151\n    KL Div: 0.0082\n    Entropy: 0.2027\n    Avg Reward: -0.2954\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:02<10:53, 24.22s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0139\n    Value Loss: 0.0179\n    KL Div: 0.0073\n    Entropy: 0.1169\n    Avg Reward: -0.2605\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0072\n    Value Loss: 0.0058\n    KL Div: 0.0021\n    Entropy: 0.1184\n    Avg Reward: -0.2605\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: 0.0000\n    Value Loss: 0.0059\n    KL Div: 0.0037\n    Entropy: 0.1039\n    Avg Reward: -0.2605\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:26<10:28, 24.17s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0040\n    Value Loss: 0.0020\n    KL Div: 0.0024\n    Entropy: 0.1032\n    Avg Reward: -0.2605\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:03<06:49, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0022\n    Value Loss: 0.0534\n    KL Div: 0.0004\n    Entropy: 0.0592\n    Avg Reward: -0.1731\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: 0.0036\n    Value Loss: 0.0475\n    KL Div: 0.0010\n    Entropy: 0.0600\n    Avg Reward: -0.1731\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: 0.0024\n    Value Loss: 0.0476\n    KL Div: 0.0007\n    Entropy: 0.0602\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:27<06:25, 24.12s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: 0.0020\n    Value Loss: 0.0505\n    KL Div: 0.0020\n    Entropy: 0.0603\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:04<02:48, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0067\n    Value Loss: 0.0009\n    KL Div: 0.0007\n    Entropy: 0.0625\n    Avg Reward: -0.2537\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0082\n    Value Loss: 0.0010\n    KL Div: 0.0007\n    Entropy: 0.0604\n    Avg Reward: -0.2537\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: 0.0038\n    Value Loss: 0.0020\n    KL Div: 0.0005\n    Entropy: 0.0595\n    Avg Reward: -0.2537\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:28<02:24, 24.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: 0.0031\n    Value Loss: 0.0015\n    KL Div: 0.0006\n    Entropy: 0.0664\n    Avg Reward: -0.2537\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:34<00:00, 23.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 2 Average Reward: -0.1908\n==================================================\n\n-----------------------------\nEpoch 3/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0068\n    Value Loss: 0.0485\n    KL Div: 0.0088\n    Entropy: 0.0828\n    Avg Reward: -0.1857\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: -0.0005\n    Value Loss: 0.0479\n    KL Div: 0.0598\n    Entropy: 0.1934\n    Avg Reward: -0.1857\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: -0.0007\n    Value Loss: 0.0638\n    KL Div: 0.1004\n    Entropy: 0.2449\n    Avg Reward: -0.1857\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:32, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: 0.0038\n    Value Loss: 0.0640\n    KL Div: 0.0118\n    Entropy: 0.1129\n    Avg Reward: -0.1857\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:01<14:51, 24.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0002\n    Value Loss: 0.1237\n    KL Div: 0.0009\n    Entropy: 0.1514\n    Avg Reward: -0.1110\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0032\n    Value Loss: 0.0962\n    KL Div: -0.0107\n    Entropy: 0.0975\n    Avg Reward: -0.1110\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0008\n    Value Loss: 0.1132\n    KL Div: -0.0120\n    Entropy: 0.0788\n    Avg Reward: -0.1110\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:25<14:28, 24.12s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0015\n    Value Loss: 0.0986\n    KL Div: -0.0045\n    Entropy: 0.0949\n    Avg Reward: -0.1110\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:02<10:51, 24.13s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0022\n    Value Loss: 0.1063\n    KL Div: 0.0010\n    Entropy: 0.0697\n    Avg Reward: -0.1002\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0031\n    Value Loss: 0.0941\n    KL Div: 0.0023\n    Entropy: 0.0761\n    Avg Reward: -0.1002\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: 0.0016\n    Value Loss: 0.0951\n    KL Div: 0.0040\n    Entropy: 0.0848\n    Avg Reward: -0.1002\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:26<10:27, 24.13s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0009\n    Value Loss: 0.0977\n    KL Div: 0.0073\n    Entropy: 0.0892\n    Avg Reward: -0.1002\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:03<06:49, 24.06s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0121\n    Value Loss: 0.0474\n    KL Div: 0.0001\n    Entropy: 0.1262\n    Avg Reward: -0.2620\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: 0.0091\n    Value Loss: 0.0101\n    KL Div: -0.0025\n    Entropy: 0.1257\n    Avg Reward: -0.2620\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: -0.0004\n    Value Loss: 0.0274\n    KL Div: 0.0076\n    Entropy: 0.1011\n    Avg Reward: -0.2620\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:27<06:24, 24.06s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: -0.0078\n    Value Loss: 0.0103\n    KL Div: 0.0164\n    Entropy: 0.1099\n    Avg Reward: -0.2620\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:04<02:48, 24.07s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0000\n    Value Loss: 0.0444\n    KL Div: 0.0003\n    Entropy: 0.0658\n    Avg Reward: -0.1731\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0003\n    Value Loss: 0.0416\n    KL Div: -0.0001\n    Entropy: 0.0637\n    Avg Reward: -0.1731\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: -0.0010\n    Value Loss: 0.0453\n    KL Div: -0.0005\n    Entropy: 0.0593\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:28<02:24, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0035\n    Value Loss: 0.0449\n    KL Div: -0.0010\n    Entropy: 0.0578\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:34<00:00, 23.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 3 Average Reward: -0.1832\n==================================================\n\n-----------------------------\nEpoch 4/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0026\n    Value Loss: 0.0039\n    KL Div: 0.0011\n    Entropy: 0.0994\n    Avg Reward: -0.2605\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: -0.0011\n    Value Loss: 0.0040\n    KL Div: 0.0077\n    Entropy: 0.1046\n    Avg Reward: -0.2605\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: -0.0016\n    Value Loss: 0.0047\n    KL Div: 0.0058\n    Entropy: 0.1032\n    Avg Reward: -0.2605\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:30, 24.14s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: -0.0044\n    Value Loss: 0.0030\n    KL Div: 0.0013\n    Entropy: 0.1063\n    Avg Reward: -0.2605\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:01<14:51, 24.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0112\n    Value Loss: 0.0023\n    KL Div: 0.0001\n    Entropy: 0.0623\n    Avg Reward: -0.2537\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0058\n    Value Loss: 0.0025\n    KL Div: -0.0003\n    Entropy: 0.0586\n    Avg Reward: -0.2537\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0047\n    Value Loss: 0.0014\n    KL Div: 0.0009\n    Entropy: 0.0577\n    Avg Reward: -0.2537\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:25<14:27, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0041\n    Value Loss: 0.0004\n    KL Div: 0.0010\n    Entropy: 0.0576\n    Avg Reward: -0.2537\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:02<10:50, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0021\n    Value Loss: 0.0472\n    KL Div: 0.0000\n    Entropy: 0.0508\n    Avg Reward: -0.1748\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0038\n    Value Loss: 0.0510\n    KL Div: 0.0000\n    Entropy: 0.0506\n    Avg Reward: -0.1748\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: -0.0004\n    Value Loss: 0.0490\n    KL Div: 0.0001\n    Entropy: 0.0506\n    Avg Reward: -0.1748\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:26<10:26, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0019\n    Value Loss: 0.0477\n    KL Div: 0.0001\n    Entropy: 0.0508\n    Avg Reward: -0.1748\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:02<06:49, 24.07s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0000\n    Value Loss: 0.0805\n    KL Div: 0.0001\n    Entropy: 0.0503\n    Avg Reward: -0.0906\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: -0.0010\n    Value Loss: 0.0905\n    KL Div: 0.0014\n    Entropy: 0.0502\n    Avg Reward: -0.0906\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: -0.0012\n    Value Loss: 0.0640\n    KL Div: -0.0000\n    Entropy: 0.0501\n    Avg Reward: -0.0906\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:26<06:24, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: 0.0028\n    Value Loss: 0.0826\n    KL Div: -0.0000\n    Entropy: 0.0504\n    Avg Reward: -0.0906\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:03<02:48, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0032\n    Value Loss: 0.0516\n    KL Div: 0.0001\n    Entropy: 0.0514\n    Avg Reward: -0.1711\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0012\n    Value Loss: 0.0474\n    KL Div: -0.0000\n    Entropy: 0.0507\n    Avg Reward: -0.1711\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: 0.0017\n    Value Loss: 0.0480\n    KL Div: -0.0000\n    Entropy: 0.0510\n    Avg Reward: -0.1711\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:27<02:24, 24.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0003\n    Value Loss: 0.0481\n    KL Div: 0.0005\n    Entropy: 0.0508\n    Avg Reward: -0.1711\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:33<00:00, 23.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 4 Average Reward: -0.1753\n==================================================\n\n-----------------------------\nEpoch 5/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0011\n    Value Loss: 0.1063\n    KL Div: -0.0001\n    Entropy: 0.0529\n    Avg Reward: -0.0922\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: 1.8145\n    Value Loss: 0.1004\n    KL Div: -0.0003\n    Entropy: 0.0517\n    Avg Reward: -0.0922\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0147\n    Value Loss: 0.0981\n    KL Div: -0.0002\n    Entropy: 0.0517\n    Avg Reward: -0.0922\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:27, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: -0.0013\n    Value Loss: 0.0913\n    KL Div: -0.0002\n    Entropy: 0.0516\n    Avg Reward: -0.0922\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:01<14:51, 24.11s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0106\n    Value Loss: 0.0015\n    KL Div: 0.0001\n    Entropy: 0.0489\n    Avg Reward: -0.2551\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0017\n    Value Loss: 0.0017\n    KL Div: 0.0000\n    Entropy: 0.0487\n    Avg Reward: -0.2551\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0013\n    Value Loss: 0.0022\n    KL Div: 0.0000\n    Entropy: 0.0487\n    Avg Reward: -0.2551\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:25<14:27, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0032\n    Value Loss: 0.0028\n    KL Div: 0.0000\n    Entropy: 0.0489\n    Avg Reward: -0.2551\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:02<10:50, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0046\n    Value Loss: 0.0495\n    KL Div: 0.0000\n    Entropy: 0.0491\n    Avg Reward: -0.1747\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 12.9168\n    Value Loss: 0.0447\n    KL Div: 0.0000\n    Entropy: 0.0491\n    Avg Reward: -0.1747\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: 0.0014\n    Value Loss: 0.0470\n    KL Div: 0.0000\n    Entropy: 0.0489\n    Avg Reward: -0.1747\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:26<10:26, 24.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0022\n    Value Loss: 0.0430\n    KL Div: 0.0000\n    Entropy: 0.0489\n    Avg Reward: -0.1747\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:03<06:49, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0185\n    Value Loss: 0.0176\n    KL Div: 0.0002\n    Entropy: 0.0482\n    Avg Reward: -0.2532\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: 0.0109\n    Value Loss: 0.0199\n    KL Div: 0.0001\n    Entropy: 0.0481\n    Avg Reward: -0.2532\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: 0.0060\n    Value Loss: 0.0138\n    KL Div: 0.0001\n    Entropy: 0.0482\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:27<06:25, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: 0.0052\n    Value Loss: 0.0085\n    KL Div: 0.0000\n    Entropy: 0.0483\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:03<02:48, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0089\n    Value Loss: 0.0030\n    KL Div: 0.0007\n    Entropy: 0.0606\n    Avg Reward: -0.2532\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0056\n    Value Loss: 0.0070\n    KL Div: 0.0020\n    Entropy: 0.0730\n    Avg Reward: -0.2532\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: 0.0028\n    Value Loss: 0.0050\n    KL Div: 0.0026\n    Entropy: 0.0601\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:28<02:24, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0002\n    Value Loss: 0.0039\n    KL Div: 0.0004\n    Entropy: 0.0614\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:33<00:00, 23.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 5 Average Reward: -0.1753\n==================================================\n\n-----------------------------\nEpoch 6/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0002\n    Value Loss: 0.1104\n    KL Div: -0.0001\n    Entropy: 0.0518\n    Avg Reward: -0.1736\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: -0.0006\n    Value Loss: 0.0408\n    KL Div: -0.0001\n    Entropy: 0.0515\n    Avg Reward: -0.1736\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0002\n    Value Loss: 0.0561\n    KL Div: -0.0002\n    Entropy: 0.0507\n    Avg Reward: -0.1736\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:28, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: -0.0011\n    Value Loss: 0.0410\n    KL Div: -0.0002\n    Entropy: 0.0503\n    Avg Reward: -0.1736\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:00<14:50, 24.06s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0056\n    Value Loss: 0.0023\n    KL Div: 0.0001\n    Entropy: 0.0488\n    Avg Reward: -0.2507\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0071\n    Value Loss: 0.0020\n    KL Div: 0.0001\n    Entropy: 0.0487\n    Avg Reward: -0.2507\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0070\n    Value Loss: 0.0047\n    KL Div: 0.0001\n    Entropy: 0.0492\n    Avg Reward: -0.2507\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:24<14:26, 24.07s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0034\n    Value Loss: 0.0042\n    KL Div: 0.0002\n    Entropy: 0.0499\n    Avg Reward: -0.2507\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:01<10:50, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0024\n    Value Loss: 0.0057\n    KL Div: 0.0001\n    Entropy: 0.0506\n    Avg Reward: -0.2544\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0004\n    Value Loss: 0.0035\n    KL Div: 0.0003\n    Entropy: 0.0526\n    Avg Reward: -0.2544\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: -0.0011\n    Value Loss: 0.0040\n    KL Div: 0.0019\n    Entropy: 0.0599\n    Avg Reward: -0.2544\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:25<10:26, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: -0.0038\n    Value Loss: 0.0018\n    KL Div: 0.0018\n    Entropy: 0.0567\n    Avg Reward: -0.2544\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:02<06:48, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: -0.0009\n    Value Loss: 0.0743\n    KL Div: 0.0001\n    Entropy: 0.0496\n    Avg Reward: -0.0898\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: -0.0007\n    Value Loss: 0.0695\n    KL Div: 0.0004\n    Entropy: 0.0500\n    Avg Reward: -0.0898\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: -0.0005\n    Value Loss: 0.0665\n    KL Div: 0.0004\n    Entropy: 0.0501\n    Avg Reward: -0.0898\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:26<06:24, 24.06s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: -0.0027\n    Value Loss: 0.0580\n    KL Div: 0.0008\n    Entropy: 0.0510\n    Avg Reward: -0.0898\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:02<02:48, 24.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0059\n    Value Loss: 0.0033\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.2532\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: 0.0093\n    Value Loss: 0.0023\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.2532\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: 0.0052\n    Value Loss: 0.0031\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:26<02:24, 24.02s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: 0.0032\n    Value Loss: 0.0026\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.2532\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:32<00:00, 23.67s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 6 Average Reward: -0.1753\n==================================================\n\n-----------------------------\nEpoch 7/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0050\n    Value Loss: 0.0856\n    KL Div: 0.0000\n    Entropy: 0.0472\n    Avg Reward: -0.0911\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: 0.0032\n    Value Loss: 0.0864\n    KL Div: 0.0000\n    Entropy: 0.0470\n    Avg Reward: -0.0911\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0021\n    Value Loss: 0.0927\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.0911\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:30, 24.14s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: 0.0025\n    Value Loss: 0.0800\n    KL Div: 0.0000\n    Entropy: 0.0471\n    Avg Reward: -0.0911\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:01<14:54, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0021\n    Value Loss: 0.0470\n    KL Div: 0.0000\n    Entropy: 0.0468\n    Avg Reward: -0.1731\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0018\n    Value Loss: 0.0550\n    KL Div: 0.0000\n    Entropy: 0.0469\n    Avg Reward: -0.1731\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: -0.0005\n    Value Loss: 0.0487\n    KL Div: 0.0000\n    Entropy: 0.0468\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:25<14:29, 24.14s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: 0.0003\n    Value Loss: 0.0478\n    KL Div: 0.0000\n    Entropy: 0.0468\n    Avg Reward: -0.1731\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:02<10:51, 24.13s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: -0.0001\n    Value Loss: 0.0340\n    KL Div: 0.0005\n    Entropy: 0.0571\n    Avg Reward: -0.2520\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: -0.0014\n    Value Loss: 0.0161\n    KL Div: 0.0016\n    Entropy: 0.0649\n    Avg Reward: -0.2520\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: -0.0005\n    Value Loss: 0.0128\n    KL Div: 0.0495\n    Entropy: 0.1489\n    Avg Reward: -0.2520\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:26<10:27, 24.13s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0200\n    Value Loss: 0.0054\n    KL Div: 0.1227\n    Entropy: 0.1862\n    Avg Reward: -0.2520\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:03<06:49, 24.10s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0035\n    Value Loss: 0.0438\n    KL Div: 0.0014\n    Entropy: 0.0753\n    Avg Reward: -0.1741\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: 0.0042\n    Value Loss: 0.0325\n    KL Div: 0.0027\n    Entropy: 0.0799\n    Avg Reward: -0.1741\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: 0.0048\n    Value Loss: 0.0258\n    KL Div: 0.0018\n    Entropy: 0.0749\n    Avg Reward: -0.1741\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:27<06:25, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: 0.0037\n    Value Loss: 0.0209\n    KL Div: 0.0015\n    Entropy: 0.0730\n    Avg Reward: -0.1741\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:05<02:49, 24.21s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: 0.0029\n    Value Loss: 0.0089\n    KL Div: 0.0000\n    Entropy: 0.0515\n    Avg Reward: -0.2512\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: -0.0010\n    Value Loss: 0.0030\n    KL Div: 0.0000\n    Entropy: 0.0517\n    Avg Reward: -0.2512\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: -0.0041\n    Value Loss: 0.0034\n    KL Div: 0.0001\n    Entropy: 0.0516\n    Avg Reward: -0.2512\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:29<02:25, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0012\n    Value Loss: 0.0014\n    KL Div: 0.0001\n    Entropy: 0.0520\n    Avg Reward: -0.2512\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:34<00:00, 23.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 7 Average Reward: -0.1747\n==================================================\n\n-----------------------------\nEpoch 8/8\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 1/4\n    Policy Loss: 0.0031\n    Value Loss: 0.0905\n    KL Div: 0.0000\n    Entropy: 0.0510\n    Avg Reward: -0.0908\n\n  Batch 0 - PPO Epoch 2/4\n    Policy Loss: 0.0104\n    Value Loss: 0.0803\n    KL Div: -0.0000\n    Entropy: 0.0510\n    Avg Reward: -0.0908\n\n  Batch 0 - PPO Epoch 3/4\n    Policy Loss: 0.0033\n    Value Loss: 0.0790\n    KL Div: 0.0000\n    Entropy: 0.0511\n    Avg Reward: -0.0908\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:   2%|▏         | 1/47 [00:24<18:32, 24.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 0 - PPO Epoch 4/4\n    Policy Loss: 0.0005\n    Value Loss: 0.0562\n    KL Div: 0.0001\n    Entropy: 0.0513\n    Avg Reward: -0.0908\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  21%|██▏       | 10/47 [04:00<14:51, 24.08s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 1/4\n    Policy Loss: 0.0024\n    Value Loss: 0.0094\n    KL Div: 0.0001\n    Entropy: 0.0490\n    Avg Reward: -0.2539\n\n  Batch 10 - PPO Epoch 2/4\n    Policy Loss: 0.0247\n    Value Loss: 0.0107\n    KL Div: 0.0000\n    Entropy: 0.0490\n    Avg Reward: -0.2539\n\n  Batch 10 - PPO Epoch 3/4\n    Policy Loss: 0.0002\n    Value Loss: 0.0035\n    KL Div: 0.0000\n    Entropy: 0.0496\n    Avg Reward: -0.2539\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  23%|██▎       | 11/47 [04:24<14:28, 24.11s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 10 - PPO Epoch 4/4\n    Policy Loss: -0.0003\n    Value Loss: 0.0016\n    KL Div: 0.0001\n    Entropy: 0.0498\n    Avg Reward: -0.2539\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  43%|████▎     | 20/47 [08:01<10:48, 24.02s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 1/4\n    Policy Loss: 0.0070\n    Value Loss: 0.0048\n    KL Div: 0.0000\n    Entropy: 0.0480\n    Avg Reward: -0.2581\n\n  Batch 20 - PPO Epoch 2/4\n    Policy Loss: 0.0022\n    Value Loss: 0.0040\n    KL Div: 0.0000\n    Entropy: 0.0479\n    Avg Reward: -0.2581\n\n  Batch 20 - PPO Epoch 3/4\n    Policy Loss: 0.0005\n    Value Loss: 0.0018\n    KL Div: 0.0002\n    Entropy: 0.0483\n    Avg Reward: -0.2581\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  45%|████▍     | 21/47 [08:25<10:24, 24.03s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 20 - PPO Epoch 4/4\n    Policy Loss: 0.0001\n    Value Loss: 0.0006\n    KL Div: 0.0006\n    Entropy: 0.0492\n    Avg Reward: -0.2581\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  64%|██████▍   | 30/47 [12:02<06:48, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 1/4\n    Policy Loss: 0.0014\n    Value Loss: 0.1733\n    KL Div: 0.0000\n    Entropy: 0.0480\n    Avg Reward: -0.0086\n\n  Batch 30 - PPO Epoch 2/4\n    Policy Loss: -0.0026\n    Value Loss: 0.1350\n    KL Div: 0.0003\n    Entropy: 0.0486\n    Avg Reward: -0.0086\n\n  Batch 30 - PPO Epoch 3/4\n    Policy Loss: -0.0044\n    Value Loss: 0.1167\n    KL Div: 0.0003\n    Entropy: 0.0491\n    Avg Reward: -0.0086\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  66%|██████▌   | 31/47 [12:26<06:24, 24.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 30 - PPO Epoch 4/4\n    Policy Loss: -0.0018\n    Value Loss: 0.1224\n    KL Div: 0.0003\n    Entropy: 0.0489\n    Avg Reward: -0.0086\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  85%|████████▌ | 40/47 [16:02<02:48, 24.01s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 1/4\n    Policy Loss: -0.0007\n    Value Loss: 0.0335\n    KL Div: 0.0000\n    Entropy: 0.0474\n    Avg Reward: -0.1735\n\n  Batch 40 - PPO Epoch 2/4\n    Policy Loss: -0.0019\n    Value Loss: 0.0354\n    KL Div: 0.0000\n    Entropy: 0.0475\n    Avg Reward: -0.1735\n\n  Batch 40 - PPO Epoch 3/4\n    Policy Loss: -0.0015\n    Value Loss: 0.0280\n    KL Div: 0.0000\n    Entropy: 0.0475\n    Avg Reward: -0.1735\n","output_type":"stream"},{"name":"stderr","text":"Rollouts:  87%|████████▋ | 41/47 [16:26<02:24, 24.02s/it]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 40 - PPO Epoch 4/4\n    Policy Loss: -0.0031\n    Value Loss: 0.0245\n    KL Div: 0.0000\n    Entropy: 0.0475\n    Avg Reward: -0.1735\n","output_type":"stream"},{"name":"stderr","text":"Rollouts: 100%|██████████| 47/47 [18:31<00:00, 23.65s/it]","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nEpoch 8 Average Reward: -0.1628\n==================================================\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"trained_policy.save_pretrained('/kaggle/working/best_policy/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:49:35.532990Z","iopub.execute_input":"2025-11-02T14:49:35.533727Z","iopub.status.idle":"2025-11-02T14:49:35.882764Z","shell.execute_reply.started":"2025-11-02T14:49:35.533699Z","shell.execute_reply":"2025-11-02T14:49:35.881924Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"!zip -r '/kaggle/working/best_policy' '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:52:25.790474Z","iopub.execute_input":"2025-11-02T14:52:25.790782Z","iopub.status.idle":"2025-11-02T14:52:26.154098Z","shell.execute_reply.started":"2025-11-02T14:52:25.790759Z","shell.execute_reply":"2025-11-02T14:52:26.153315Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/best_policy/ (stored 0%)\n  adding: kaggle/working/best_policy/README.md (deflated 65%)\n  adding: kaggle/working/best_policy/adapter_config.json (deflated 55%)\n  adding: kaggle/working/best_policy/adapter_model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"test_prompt = \"What is 2 + 1?\"\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to('cuda')\noutput = trained_policy.generate(\n    **inputs, \n    max_new_tokens=3,\n    do_sample=False  # Greedy for testing\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:53:20.433898Z","iopub.execute_input":"2025-11-02T14:53:20.434219Z","iopub.status.idle":"2025-11-02T14:53:20.584186Z","shell.execute_reply.started":"2025-11-02T14:53:20.434192Z","shell.execute_reply":"2025-11-02T14:53:20.583495Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What is 2 + 1?\n-2\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tune_ppo_trainer(\n    policy_model: PeftModel, \n    value_model: PeftModel,\n    ref_model: PeftModel,\n    tokenizer: PreTrainedTokenizerBase,\n    dataset: Dataset,\n    policy_optimizer: torch.optim.AdamW,\n    value_optimizer: torch.optim.AdamW,\n    reward_model: Optional[Callable[[list[str], list[str]], list[float]]] = None,\n    num_epochs=3,\n    rollout_batch_size=32,\n    mini_batch_size=8,\n    ppo_epochs=4,\n    clip_eps: float=0.1,\n    kl_coef: float=0.01,\n    value_coef: float=0.5,\n    entropy_coef: float=0.01,\n    gamma: float=0.99,\n    lam: float=0.95,\n    max_gen_len: int=128\n):\n    def collate_fn(batch):\n        input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n        attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n        answers = [item['answer'] for item in batch]\n        questions = [item['question'] for item in batch]\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'answers': answers,\n            'questions': questions\n        }\n        \n    dataloader = DataLoader(dataset, batch_size=rollout_batch_size, collate_fn=collate_fn,shuffle=True)\n    all_epoch_rewards = []\n\n    for epoch in range(num_epochs):\n        print('-----------------------------')\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        for b_idx, batch in enumerate(tqdm(dataloader, desc='Rollouts')):\n            policy_model.eval()\n            value_model.eval()\n            ref_model.eval()\n            \n            rollout_buffer=[]\n            epoch_rewards = []\n\n            with torch.no_grad():\n                input_ids = batch['input_ids'].to(policy_model.device)\n                attention_mask = batch['attention_mask'].to(policy_model.device)\n                answers = batch['answers']\n                \n                response_token = policy_model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_new_tokens=max_gen_len,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    pad_token_id=tokenizer.pad_token_id\n                )\n                responses=tokenizer.batch_decode(response_token, skip_special_tokens=True)\n                \n                rewards = compute_reward(responses, answers)\n                rewards = rewards.to(policy_model.device)\n                batch_avg_reward = rewards.mean().item()\n                epoch_rewards.append(batch_avg_reward)\n\n                attention_mask = (response_token != tokenizer.pad_token_id).long()\n\n                values = value_model(\n                    input_ids=response_token,\n                    attention_mask=attention_mask\n                ).logits.squeeze(-1)\n\n                old_log_probs, mask = get_log_probs(\n                    policy_model, response_token, attention_mask\n                )\n                ref_log_probs,_ = get_log_probs(\n                    ref_model, response_token, attention_mask\n                )\n                advantages, returns = compute_gae(rewards, values, gamma, lam)\n                advantages = (advantages-advantages.mean()) / (advantages.std()+1e-8)\n\n                for i in range(len(batch)):\n                    rollout_buffer.append({\n                        'input_ids': response_token[i].cpu(),\n                        'attention_mask': attention_mask[i].cpu(),\n                        'old_log_probs': old_log_probs[i].cpu(),\n                        'ref_log_probs': ref_log_probs[i].cpu(),\n                        'advantages': advantages[i].cpu(),\n                        'returns': returns[i].cpu(),\n                        'mask': mask[i].cpu()\n                    })\n            policy_model.train()\n            value_model.train()\n\n            for ppo_epoch in range(ppo_epochs):\n                import random\n                random.shuffle(rollout_buffer)\n                num_minibatches = len(rollout_buffer)//mini_batch_size\n\n                ppo_policy_losses =[]\n                ppo_value_losses =[]\n                ppo_kl_penalties = []\n                ppo_entropies = []\n                for mb_idx in range(num_minibatches):\n                    start_idx = mb_idx* mini_batch_size\n                    end_idx = start_idx + mini_batch_size \n                    minibatch = rollout_buffer[start_idx:end_idx]\n                    \n                    #stack minibatches\n                    mb_input_ids = torch.stack([item['input_ids']for item in minibatch])\n                    mb_attention_mask = torch.stack([item['attention_mask'] for item in minibatch])\n                    mb_old_log_probs = torch.stack([item['old_log_probs'] for item in minibatch])\n                    mb_ref_log_probs = torch.stack([item['ref_log_probs'] for item in minibatch])\n                    mb_advantages = torch.stack([item['advantages'] for item in minibatch])\n                    mb_returns = torch.stack([item['returns'] for item in minibatch])\n                    mb_mask = torch.stack([item['mask'] for item in minibatch])\n\n                    #move to device\n                    mb_input_ids = mb_input_ids.to(policy_model.device)\n                    mb_attention_mask = mb_attention_mask.to(policy_model.device)\n                    mb_advantages = mb_advantages.to(policy_model.device)\n                    mb_old_log_probs = mb_old_log_probs.to(policy_model.device)\n                    mb_ref_log_probs = mb_ref_log_probs.to(policy_model.device)\n                    mb_returns = mb_returns.to(policy_model.device)\n                    mb_mask = mb_mask.to(policy_model.device)\n                    \n                    policy_optimizer.zero_grad()\n                    new_log_probs, _ = get_log_probs(\n                        policy_model,\n                        mb_input_ids,\n                        mb_attention_mask\n                    )\n                    \n                    #PPO loss\n                    ratio = torch.exp(new_log_probs - mb_old_log_probs)    \n                    surr1 = ratio*mb_advantages*mb_mask\n                    surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps)* mb_advantages * mb_mask\n                    policy_loss = -torch.min(surr1, surr2).sum() / mb_mask.sum()\n                    #KL \n                    kl_div = (new_log_probs - mb_ref_log_probs)*mb_mask\n                    kl_penalty = kl_div.sum() / mb_mask.sum()\n                    #entropy\n                    outputs = policy_model(\n                        input_ids=mb_input_ids,\n                        attention_mask=mb_attention_mask\n                    )\n                    \n                    entropy=compute_entropy(outputs.logits[:, :-1, :], mb_mask)\n\n                    total_policy_loss = policy_loss + kl_coef * kl_penalty - entropy_coef * entropy.mean()\n                    #backward\n                    if not torch.isnan(total_policy_loss):\n                        total_policy_loss.backward()\n                        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n                        policy_optimizer.step()\n\n                    #value update\n                    value_optimizer.zero_grad()\n                    new_values = value_model(\n                        input_ids=mb_input_ids,\n                        attention_mask=mb_attention_mask\n                    ).logits.squeeze(-1)\n                    \n                    # if new_values.dim() == 1:\n                    #     seq_len = mb_input_ids.shape[1]\n                    #     new_values = new_values.unsqueeze(-1).expand(-1, seq_len)\n                    final_returns = mb_returns[:, -1]\n                    #value loss\n                    # value_loss = ((new_values-final_returns)**2*mb_mask).sum()/ mb_mask.sum()\n                    value_loss = value_coef * ((new_values - final_returns) ** 2).mean()\n\n\n                    # Backward and optimize\n                    if not torch.isnan(value_loss):\n                        value_loss.backward()\n                        torch.nn.utils.clip_grad_norm_(value_model.parameters(), max_norm=1.0)\n                        value_optimizer.step()\n\n                    ppo_policy_losses.append(policy_loss.item())\n                    ppo_value_losses.append(value_loss.item())\n                    ppo_kl_penalties.append(kl_penalty.item())\n                    ppo_entropies.append(entropy.mean().item())\n                \n                if b_idx % 10 == 0:\n                    print(f\"\\n  Batch {b_idx} - PPO Epoch {ppo_epoch+1}/{ppo_epochs}\")\n                    print(f\"    Policy Loss: {sum(ppo_policy_losses)/len(ppo_policy_losses):.4f}\")\n                    print(f\"    Value Loss: {sum(ppo_value_losses)/len(ppo_value_losses):.4f}\")\n                    print(f\"    KL Div: {sum(ppo_kl_penalties)/len(ppo_kl_penalties):.4f}\")\n                    print(f\"    Entropy: {sum(ppo_entropies)/len(ppo_entropies):.4f}\")\n                    print(f\"    Avg Reward: {batch_avg_reward:.4f}\")\n            \n            del rollout_buffer\n            torch.cuda.empty_cache()     \n            \n        epoch_avg_reward = sum(epoch_rewards) / len(epoch_rewards)\n        all_epoch_rewards.append(epoch_avg_reward)\n        print(f\"\\n{'='*50}\")\n        print(f\"Epoch {epoch+1} Average Reward: {epoch_avg_reward:.4f}\")\n        print(f\"{'='*50}\\n\")\n    \n    final_avg_reward = sum(all_epoch_rewards) / len(all_epoch_rewards)\n    print(f\"\\n{'='*50}\")\n    print(f\"FINAL AVERAGE REWARD: {final_avg_reward:.4f}\")\n    print(f\"{'='*50}\\n\")\n\n    return final_avg_reward","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n\n    clip_eps = trial.suggest_categorical('clip_eps', [0.1, 0.15, 0.2, 0.3])\n    kl_coef = trial.suggest_categorical('kl_coef', [0.1, 0.2, 0.3, 0.4])\n    value_coef = trial.suggest_categorical('value_coef', [0.5, 0.4, 0.6, 0.3])\n    entropy_coef = trial.suggest_categorical('entropy_coef', [0.01, 0.03, 0.05, 0.04])\n    gamma = trial.suggest_categorical('gamma', [0.99, 0.90, 0.85, 0.70])\n    lam = trial.suggest_categorical('lam', [0.95, 0.87, 0.80, 0.76])\n    # rollout_batch_size = trial.suggest_categorical('rollout_batch_size'. [8, 16, 32])\n    # mini_batch_size = \n    policy_lr = trial.suggest_loguniform('policy_lr', 5e-5, 1e-3)\n    value_lr = trial.suggest_loguniform('value_lr', 5e-5, 1e-3)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"TRIAL {trial.number}\")\n    print(f\"{'='*60}\")\n    print(f\"Hyperparameters:\")\n    print(f\"  clip_eps: {clip_eps}\")\n    print(f\"  kl_coef: {kl_coef}\")\n    print(f\"  value_coef: {value_coef}\")\n    print(f\"  entropy_coef: {entropy_coef}\")\n    print(f\"  gamma: {gamma}\")\n    print(f\"  lam: {lam}\")\n    print(f\"  policy_lr: {policy_lr:.2e}\")\n    print(f\"  value_lr: {value_lr:.2e}\")\n    print(f\"{'='*60}\\n\")\n\n    trial_policy_model = PeftModel.from_pretrained(\n        base_model,\n        \"/kaggle/input/loraadapters/pytorch/default/1\",\n        trainable=True\n    )\n    \n    trial_value_model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=1,\n        torch_dtype=torch.float16\n    )\n    for n,p in trial_policy_model.named_parameters():\n        if 'lora' in n:\n            p.requires_grad=True\n            \n    trial_value_model.config.pad_token_id = tokenizer.pad_token_id\n    trial_value_model = get_peft_model(trial_value_model, value_config)\n    trial_value_model.to('cuda')\n    \n    policy_optimizer = torch.optim.AdamW(trial_policy_model.parameters(), lr=policy_lr)\n    value_optimizer = torch.optim.AdamW(trial_value_model.parameters(), lr=value_lr)\n    \n    small_dataset = tokenized_dataset.select(range(min(200, len(tokenized_dataset))))\n    try:\n        average_reward = tune_ppo_trainer(\n            policy_model=trial_policy_model,\n            value_model=trial_value_model,\n            ref_model=ref_model,\n            tokenizer=tokenizer,\n            dataset=small_dataset,\n            policy_optimizer=policy_optimizer,\n            value_optimizer=value_optimizer,\n            reward_model=None,\n            num_epochs=1,  # Just 1 epoch for tuning\n            rollout_batch_size=8,  # Smaller for faster tuning\n            mini_batch_size=4,\n            ppo_epochs=4,\n            clip_eps=clip_eps,\n            kl_coef=kl_coef,\n            value_coef=value_coef,\n            entropy_coef=entropy_coef,\n            gamma=gamma,\n            lam=lam,\n            max_gen_len=64  # Shorter generation for speed\n        )\n    except Exception as e:\n        print(f\"Trial {trial.number} failed with error: {e}\")\n        return -10.0\n    finally:\n        del trial_policy_model, trial_value_model\n        torch.cuda.empty_cache()\n    return average_reward","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting Optuna hyperparameter search...\")\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20, timeout=3600*6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = study.best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=best_params['policy_lr'])\nvalue_optimizer = torch.optim.AdamW(value_model.parameters(), lr=best_params['value_lr'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_policy, trained_value = ppo_trainer(\n    policy_model=trial_policy_model,\n    value_model=trial_value_model,\n    ref_model=ref_model,\n    tokenizer=tokenizer,\n    dataset=small_dataset,\n    policy_optimizer=policy_optimizer,\n    value_optimizer=value_optimizer,\n    reward_model=None,\n    num_epochs=8,  \n    rollout_batch_size=32,  \n    mini_batch_size=8,\n    ppo_epochs=4,\n    clip_eps=best_params['clip_eps'],\n    kl_coef=best_params['kl_coef'],\n    value_coef=best_params['value_coef'],\n    entropy_coef=best_params['entropy_coef'],\n    gamma=best_params['gamma'],\n    lam=best_params['lam'],\n    max_gen_len=128 \n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}